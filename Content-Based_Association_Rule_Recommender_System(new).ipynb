{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install efficient-apriori \n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom efficient_apriori import apriori # the apriori algorithm (finds association rules in a series of transactions)\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, we import all of our data into Pandas dataframes."},{"metadata":{"trusted":true},"cell_type":"code","source":"booksRaw =  pd.read_csv(\"../input/books.csv\")\ntagsRaw = pd.read_csv(\"../input/tags.csv\")\nbookTagsRaw = pd.read_csv(\"../input/book_tags.csv\")\nratingsRaw = pd.read_csv(\"../input/ratings.csv\")\nto_readRaw = pd.read_csv(\"../input/to_read.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We merge bookTags with tags, which associates tag names with the book tags. Then, we group the tags by book_id to streamline the process of making a keyword list for each book."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_tags = pd.merge(bookTagsRaw, tagsRaw, left_on='tag_id', right_on='tag_id', how='inner')\n#all_tags = pd.concat([all_tags, booksRaw[['book_id','authors']].rename(columns={'book_id':'goodreads_book_id'})])\nall_tags_grouped = all_tags.groupby(\"goodreads_book_id\")\n\n#all_tags[all_tags.goodreads_book_id == 10]\nall_tags.tail(100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We group the ratings by users in preparation for making the user profiles."},{"metadata":{"trusted":true},"cell_type":"code","source":"ratingsGrouped = ratingsRaw.sort_values('user_id').groupby(by='user_id')\nratingsGrouped.get_group(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time to make the user profiles. Each profile has a list of keyword lists for all rated books. We also add a special \"like\" or \"dislike\" word to each list of keywords that depends on the user's rating of the book. \n\nWe also keep two aggregate lists of all keywords of high-rated books and low-rated books. We remove the intersection of the two from the list of all tags. If both a 5-star book and 1-star book are both tagged with \"sci-fi\", we can't say much about how the user rates sci-fi books."},{"metadata":{"trusted":true},"cell_type":"code","source":"class UserProfile:\n  def __init__(self, user_id):\n    self.userID = user_id;\n    self.allTags = [];\n    self.highWords = set()\n    self.lowWords = set()\n    \n  def makeUserProfile(self):\n      ratedBooks = ratingsGrouped.get_group(self.userID)\n      high = ratedBooks[ratedBooks.rating >= 3]\n      low = ratedBooks[ratedBooks.rating < 3]\n      \n      print(\"Highly rated: \", high)\n      print(\"Didn't think much of these: \", low)\n      for index, row in high.iterrows():\n        #Add tags of current book to list of keywords\n        taglist = all_tags[all_tags.goodreads_book_id == row['book_id']].tag_name.tolist()\n        \n        #Add author names to list of keywords for current book\n        authors = booksRaw[booksRaw.book_id == row['book_id']].authors.tolist()\n        \n        if (authors != []):\n          authors = authors[0].split(\",\")\n          taglist.extend(authors)\n            \n        self.highWords.union(set(taglist))\n        taglist.append(\"like\")\n        if (taglist != []):\n          self.allTags.append(taglist)\n      #print(self.allTags)\n      for index, row in low.iterrows():\n        print(\"check\")\n        taglist = all_tags[all_tags.goodreads_book_id == row['book_id']].tag_name.tolist()\n        \n        authors = booksRaw[booksRaw.book_id == row['book_id']].authors.tolist()\n        \n        if (authors != []):\n          authors = authors[0].split(\",\")\n          taglist.extend(authors)\n        \n        self.lowWords.union(set(taglist))\n        taglist.append(\"dislike\")\n        if (taglist != []):\n          self.allTags.append(taglist)\n      \n      #print(self.allTags)\n      intersection = self.highWords.intersection(self.lowWords)\n      for i in self.allTags:\n        j = 0\n        while j < len(i):\n          #print(i)\n          #print(j)\n          if i[j] in intersection:\n            i.pop(j)\n          j += 1\n\nuser2 = UserProfile(17566)\nuser2.makeUserProfile()\nprint(user2.allTags)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use the apriori algorithm to find association rules across all the keyword lists. It essentially works by finding frequent sets of words and keeping track of how often they appear together."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Length of highTags: \", sum(len(tag) for tag in user2.allTags))\nallItemsets, allRules = list(apriori(user2.allTags, min_confidence=0.3,min_support=0.13))\nprint(\"Without minimum confidence and support, there would be over rules! Here we narrow it down to just\", len(allRules),\".\")\nprint(allRules)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We only keep the rules that associate sets of keywords with high or low ratings, and we use these rules to find relevant books (books that fire many rules)."},{"metadata":{"trusted":true},"cell_type":"code","source":"highRules = set(filter(lambda rule: 'like' in rule.rhs and len(rule.rhs) == 1, allRules))\nlowRules = set(filter(lambda rule: 'dislike' in rule.rhs and len(rule.rhs) == 1, allRules))\n\ntotal = 0\nrules_fired = 0\npredicted_ratings = []\n\n#for book in all_tags_grouped['goodreads_book_id'].unique():\nfor book in all_tags_grouped['goodreads_book_id'].unique():\n    #print(book)\n    explanation = []\n    tags = set(all_tags_grouped.get_group(book[0])['tag_name'].unique())\n    #print(list(highRules))\n    for rule in highRules:\n        rule_words = set([key for key in rule.lhs])\n        #print(\"flag: \", book)\n        if rule_words.issubset(tags):\n            \n            explanation.extend([key for key in rule.lhs])\n            total += 1\n            \n    for rule in lowRules:\n        rule_words = set([key for key in rule.lhs])\n        \n        if rule_words.issubset(tags):\n            explanation.extend([key for key in rule.lhs])\n            total -= 1\n            \n    predicted_ratings.append([booksRaw[booksRaw[\"goodreads_book_id\"] == book[0]].title,explanation,total])\n    total = 0\n    #print(\"The predicted rating for book \", book[0], \"is \", predicted_rating)\npredicted_ratings = pd.DataFrame(predicted_ratings,columns=[\"book_name\",\"explanation\",\"predicted_rating\"])\npredicted_ratings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All thats left is to rescale the predicted ratings, and sort them in decending order. To get the top-k recommendations, just return the highest k entries of this list."},{"metadata":{"trusted":true},"cell_type":"code","source":"min = predicted_ratings[\"predicted_rating\"].min()\nmax = predicted_ratings[\"predicted_rating\"].max()\n\ndef minmax(x):\n    return (x-min)/(max-min)\n\npredicted_ratings[\"rescaled_rating\"] = predicted_ratings[\"predicted_rating\"].apply(minmax)\npredicted_ratings\npredicted_ratings.sort_values(\"rescaled_rating\",ascending=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}