{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"../input\"))\n\nimport surprise  #Scikit-Learn library for recommender systems. \n","execution_count":18,"outputs":[{"output_type":"stream","text":"['ratings.csv', 'tags.csv', 'sample_book.xml', 'book_tags.csv', 'to_read.csv', 'books.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Creating an Explicit Latent Matrix Factorization Recommender System for Books10k Dataset.\nWe're basing this off of this paper: https://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf "},{"metadata":{},"cell_type":"markdown","source":"With Surprise, Data Wrangling is almost automated. We'll load the dataset into Pandas, then leverage the surprise package "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Probabilistic Matrix Factorization is a prediction method where you estimate the best factorization for the Ratings Matrix. Taking an example, lets say there's a rating matrix of 100k users and 10k items. "},{"metadata":{"trusted":true},"cell_type":"code","source":"raw=pd.read_csv('../input/ratings.csv')\nraw.drop_duplicates(inplace=True)\nprint('we have',raw.shape[0], 'ratings')\nprint('the number of unique users we have is:', len(raw.user_id.unique()))\nprint('the number of unique books we have is:', len(raw.book_id.unique()))\nprint(\"The median user rated %d books.\"%raw.user_id.value_counts().median())\nprint('The max rating is: %d'%raw.rating.max(),\"the min rating is: %d\"%raw.rating.min())\nraw.head()","execution_count":19,"outputs":[{"output_type":"stream","text":"we have 980112 ratings\nthe number of unique users we have is: 53424\nthe number of unique books we have is: 10000\nThe median user rated 8 books.\nThe max rating is: 5 the min rating is: 1\n","name":"stdout"},{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"   book_id  user_id  rating\n0        1      314       5\n1        1      439       3\n2        1      588       5\n3        1     1169       4\n4        1     1185       4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n      <th>user_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>314</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>439</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>588</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1169</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1185</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Loading dataset into Surprise:\nFor a dataset to be loaded into surprise from a pandas dataframe, you have to specify a reader object and ensure that the columns the dataset are in the order: user,item, rating. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#swapping columns\nraw=raw[['user_id','book_id','rating']] \n# when importing from a DF, you only need to specify the scale of the ratings.\nreader = surprise.Reader(rating_scale=(1,5)) \n#into surprise:\ndata = surprise.Dataset.load_from_df(raw,reader)","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Surprise lets you implement your own algorithm while still being able to use its very powerful tools for measuring error and choosing hyperparameters. \nTo do so, we have to make a class inheriting from the AlgoBase class. We'll define it along with a fit and estimate method.\nThe fit method uses Stochastic Gradient Descent to be able to estimate the best probabilistic matrix factors."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ProbabilisticMatrixFactorization(surprise.AlgoBase):\n# Randomly initializes two Matrices, Stochastic Gradient Descent to be able to optimize the best factorization for ratings.\n    def __init__(self,learning_rate,num_epochs,num_factors):\n       # super(surprise.AlgoBase)\n        self.alpha = learning_rate #learning rate for Stochastic Gradient Descent\n        self.num_epochs = num_epochs\n        self.num_factors = num_factors\n    def fit(self,train):\n        #randomly initialize user/item factors from a Gaussian\n        P = np.random.normal(0,.1,(train.n_users,self.num_factors))\n        Q = np.random.normal(0,.1,(train.n_items,self.num_factors))\n        #print('fit')\n\n        for epoch in range(self.num_epochs):\n            for u,i,r_ui in train.all_ratings():\n                residual = r_ui - np.dot(P[u],Q[i])\n                temp = P[u,:] # we want to update them at the same time, so we make a temporary variable. \n                P[u,:] +=  self.alpha * residual * Q[i]\n                Q[i,:] +=  self.alpha * residual * temp \n\n                \n        self.P = P\n        self.Q = Q\n\n        self.trainset = train\n    \n    \n    def estimate(self,u,i):\n        #returns estimated rating for user u and item i. Prerequisite: Algorithm must be fit to training set.\n        #check to see if u and i are in the train set:\n        #print('gahh')\n\n        if self.trainset.knows_user(u) and self.trainset.knows_item(i):\n            #print(u,i, '\\n','yep:', self.P[u],self.Q[i])\n            #return scalar product of P[u] and Q[i]\n            nanCheck = np.dot(self.P[u],self.Q[i])\n            \n            if np.isnan(nanCheck):\n                return self.trainset.global_mean\n            else:\n                return np.dot(self.P[u,:],self.Q[i,:])\n        else:# if its not known we'll return the general average. \n           # print('global mean')\n            return self.trainset.global_mean\n                \n        ","execution_count":21,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we'll split the data into 2-fold cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"Alg1 = ProbabilisticMatrixFactorization(learning_rate=0.05,num_epochs=4,num_factors=10)\ndata1 = data.build_full_trainset()\nAlg1.fit(data1)\nprint(raw.user_id.iloc[4],raw.book_id.iloc[4])\nAlg1.estimate(raw.user_id.iloc[4],raw.book_id.iloc[4])","execution_count":22,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in multiply\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:18: RuntimeWarning: overflow encountered in multiply\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in add\n","name":"stderr"},{"output_type":"stream","text":"1185 1\n","name":"stdout"},{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"3.8560440031343357"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"Alg1.estimate(raw.user_id.iloc[4],raw.book_id.iloc[4])","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"3.8560440031343357"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Another cool thing about Suprise is it allows you to use their built in GridSearch, which is really similiar and inspired by sk-learn's GridSearchCV. Its a handy tool that allows us to put in a whole range of hyper-parameters, and picks the best one. \n\nLets use it!"},{"metadata":{"trusted":true},"cell_type":"code","source":"gs = surprise.model_selection.GridSearchCV(ProbabilisticMatrixFactorization, param_grid={'learning_rate':[0.005,0.01],\n                                                                            'num_epochs':[5,10],\n                                                                            'num_factors':[10,20]},measures=['rmse', 'mae'], cv=2)\ngs.fit(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('rsme: ',gs.best_score['rmse'],'mae: ',gs.best_score['mae'])\nbest_params = gs.best_params['rmse']\nprint('rsme: ',gs.best_params['rmse'],'mae: ',gs.best_params['mae'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nbestVersion = ProbabilisticMatrixFactorization(learning_rate=best_params['learning_rate'],num_epochs=best_params['num_epochs'],num_factors=best_params['num_factors'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we can use k-fold cross validation to evaluate the best model. \nkSplit = surprise.model_selection.KFold(n_splits=10,shuffle=True)\nfor train,test in kSplit.split(data):\n    bestVersion.fit(train)\n    prediction = bestVersion.test(test)\n    surprise.accuracy.rmse(prediction,verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}